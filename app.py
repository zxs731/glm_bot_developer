import streamlit as st
import os
import json, ast
import requests
from zhipuai import ZhipuAI



def python_inter(py_code, g='globals()'):
    """
    ä¸“é—¨ç”¨äºæ‰§è¡Œéç»˜å›¾ç±»pythonä»£ç ï¼Œå¹¶è·å–æœ€ç»ˆæŸ¥è¯¢æˆ–å¤„ç†ç»“æœã€‚è‹¥æ˜¯è®¾è®¡ç»˜å›¾æ“ä½œçš„Pythonä»£ç ï¼Œåˆ™éœ€è¦è°ƒç”¨fig_interå‡½æ•°æ¥æ‰§è¡Œã€‚
    :param py_code: å­—ç¬¦ä¸²å½¢å¼çš„Pythonä»£ç ï¼Œç”¨äºæ‰§è¡Œå¯¹telco_dbæ•°æ®åº“ä¸­å„å¼ æ•°æ®è¡¨è¿›è¡Œæ“ä½œ
    :param g: gï¼Œå­—ç¬¦ä¸²å½¢å¼å˜é‡ï¼Œè¡¨ç¤ºç¯å¢ƒå˜é‡ï¼Œæ— éœ€è®¾ç½®ï¼Œä¿æŒé»˜è®¤å‚æ•°å³å¯
    :returnï¼šä»£ç è¿è¡Œçš„æœ€ç»ˆç»“æœ
    """    
    
    global_vars_before = set(g.keys())
    try:
        exec(py_code, g)            
    except Exception as e:
        return f"ä»£ç æ‰§è¡Œæ—¶æŠ¥é”™{e}"
    global_vars_after = set(g.keys())
    new_vars = global_vars_after - global_vars_before
    # è‹¥å­˜åœ¨æ–°å˜é‡
    if new_vars:
        result = {var: g[var] for var in new_vars}
        return str(result)
    # è‹¥ä¸å­˜åœ¨æ–°å˜é‡ï¼Œå³æœ‰å¯èƒ½æ˜¯ä»£ç æ˜¯è¡¨è¾¾å¼ï¼Œä¹Ÿæœ‰å¯èƒ½ä»£ç å¯¹ç›¸åŒå˜é‡é‡å¤èµ‹å€¼
    else:
        try:
            # å°è¯•å¦‚æœæ˜¯è¡¨è¾¾å¼ï¼Œåˆ™è¿”å›è¡¨è¾¾å¼è¿è¡Œç»“æœ
            return str(eval(py_code, g))
        # è‹¥æŠ¥é”™ï¼Œåˆ™å…ˆæµ‹è¯•æ˜¯å¦æ˜¯å¯¹ç›¸åŒå˜é‡é‡å¤èµ‹å€¼
        except Exception as e:
            try:
                exec(py_code, g)
                return "å·²ç»é¡ºåˆ©æ‰§è¡Œä»£ç "
            except Exception as e:
                pass
            # è‹¥ä¸æ˜¯é‡å¤èµ‹å€¼ï¼Œåˆ™æŠ¥é”™
            return f"ä»£ç æ‰§è¡Œæ—¶æŠ¥é”™{e}"
            

python_inter_function_info = {
    'name': 'python_inter',
    'description': 'ä¸“é—¨ç”¨äºpythonä»£ç ï¼Œå¹¶è·å–æœ€ç»ˆæŸ¥è¯¢æˆ–å¤„ç†ç»“æœã€‚',
    'parameters': {
        'type': 'object',
        'properties': {
            'py_code': {
                'type': 'string',
                'description': 'ç”¨äºæ‰§è¡Œåœ¨æœ¬åœ°ç¯å¢ƒè¿è¡Œçš„pythonä»£ç '
            },
            'g': {
                'type': 'string',
                'description': 'ç¯å¢ƒå˜é‡ï¼Œå¯é€‰å‚æ•°ï¼Œä¿æŒé»˜è®¤å‚æ•°å³å¯'
            }
        },
        'required': ['py_code']
    }
}

functions_list = [python_inter]
available_functions = {
            "python_inter": python_inter,
        }
tools = [
    {
        "type": "function",
        "function":python_inter_function_info
    }
]


def get_glm_response(messages, 
                     tools=None, 
                     model='glm-4'):
    """
    å•æ¬¡GLMæ¨¡å‹å“åº”å‡½æ•°ï¼Œèƒ½å¤Ÿæ­£å¸¸è·å–æ¨¡å‹å“åº”ï¼Œå¹¶åœ¨æ¨¡å‹æ— æ³•æ­£å¸¸å“åº”æ—¶æš‚åœæ¨¡å‹è°ƒç”¨ï¼Œ\
    å¹¶åœ¨ä¼‘æ¯ä¸€åˆ†é’Ÿä¹‹åç»§ç»­è°ƒç”¨æ¨¡å‹ã€‚æœ€å¤šå°è¯•ä¸‰æ¬¡ã€‚
    :param messages: å¿…è¦å‚æ•°ï¼Œå­—å…¸ç±»å‹ï¼Œè¾“å…¥åˆ°Chatæ¨¡å‹çš„messageså‚æ•°å¯¹è±¡
    :param tools: å¯é€‰å‚æ•°ï¼Œé»˜è®¤ä¸ºNoneï¼Œå¯ä»¥è®¾ç½®ä¸ºåŒ…å«å…¨éƒ¨å¤–éƒ¨å‡½æ•°å‚æ•°è§£é‡ŠSchemaæ ¼å¼åˆ—è¡¨
    :param model: Chatæ¨¡å‹ï¼Œå¯é€‰å‚æ•°ï¼Œé»˜è®¤æ¨¡å‹ä¸ºglm-4
    :returnï¼šChatæ¨¡å‹è¾“å‡ºç»“æœ
    """
    attempts = 0
    max_attempts = 3

    while attempts < max_attempts:
        try:
            response = client.chat.completions.create(
                model=model,
                messages=messages[-15:],  # ä½¿ç”¨å‡½æ•°å‚æ•°
                tools=tools,
            )
            return response  # æˆåŠŸæ—¶è¿”å›å“åº”
        except Exception as e:  # æ•è·æ‰€æœ‰å¼‚å¸¸
            print(f"å‘ç”Ÿé”™è¯¯ï¼š{e}")
            attempts += 1
            if attempts < max_attempts:
                print("ç­‰å¾…1åˆ†é’Ÿåé‡è¯•...")
                time.sleep(60)  # ç­‰å¾…1åˆ†é’Ÿ
            else:
                print("å°è¯•æ¬¡æ•°è¿‡å¤šï¼Œåœæ­¢å°è¯•ã€‚")
                return None  # åœ¨æ‰€æœ‰å°è¯•åè¿”å› None
                
def check_code_run(messages, feedback,
                   functions_list = None, 
                   tools = None,
                   model = "glm-4", 
                   auto_run = True):
    
    """
    èƒ½å¤Ÿè‡ªåŠ¨æ‰§è¡Œå¤–éƒ¨å‡½æ•°è°ƒç”¨çš„Chatå¯¹è¯æ¨¡å‹ï¼Œä¸“é—¨ç”¨äºä»£ç è§£é‡Šå™¨çš„æ„å»ºè¿‡ç¨‹ï¼Œå¯ä»¥é€šè¿‡auto_runå‚æ•°è®¾ç½®ï¼Œå†³å®šæ˜¯å¦è‡ªåŠ¨æ‰§è¡Œä»£ç 
    :param messages: å¿…è¦å‚æ•°ï¼Œå­—å…¸ç±»å‹ï¼Œè¾“å…¥åˆ°Chatæ¨¡å‹çš„messageså‚æ•°å¯¹è±¡
    :param functions_list: å¯é€‰å‚æ•°ï¼Œé»˜è®¤ä¸ºNoneï¼Œå¯ä»¥è®¾ç½®ä¸ºåŒ…å«å…¨éƒ¨å¤–éƒ¨å‡½æ•°çš„åˆ—è¡¨å¯¹è±¡
    :param tools: å¯é€‰å‚æ•°ï¼Œé»˜è®¤ä¸ºNoneï¼Œå¯ä»¥è®¾ç½®ä¸ºåŒ…å«å…¨éƒ¨å¤–éƒ¨å‡½æ•°å‚æ•°è§£é‡ŠSchemaæ ¼å¼åˆ—è¡¨
    :param model: Chatæ¨¡å‹ï¼Œå¯é€‰å‚æ•°ï¼Œé»˜è®¤æ¨¡å‹ä¸ºglm-4
    :auto_runï¼šåœ¨è°ƒç”¨å¤–éƒ¨å‡½æ•°çš„æƒ…å†µä¸‹ï¼Œæ˜¯å¦è‡ªåŠ¨è¿›è¡ŒSecond Responseã€‚è¯¥å‚æ•°åªåœ¨å¤–éƒ¨å‡½æ•°å­˜åœ¨æ—¶èµ·ä½œç”¨
    :returnï¼šChatæ¨¡å‹è¾“å‡ºç»“æœ
    """
    
    # å¦‚æœæ²¡æœ‰å¤–éƒ¨å‡½æ•°åº“ï¼Œåˆ™æ‰§è¡Œæ™®é€šçš„å¯¹è¯ä»»åŠ¡
    if tools == None:
        response = get_glm_response(
            messages=messages, 
            tools=tools, 
            model=model
        )   
        response_message = response.choices[0].message
        
    # è‹¥å­˜åœ¨å¤–éƒ¨å‡½æ•°åº“ï¼Œåˆ™éœ€è¦çµæ´»é€‰å–å¤–éƒ¨å‡½æ•°å¹¶è¿›è¡Œå›ç­”
    else:
        
        # åˆ›å»ºå¤–éƒ¨å‡½æ•°åº“å­—å…¸
        available_functions = {func.__name__: func for func in functions_list}

        # first response
        response = get_glm_response(
            messages=messages, 
            tools=tools, 
            model=model
        )
        response_message = response.choices[0].message

        # åˆ¤æ–­è¿”å›ç»“æœæ˜¯å¦å­˜åœ¨function_callï¼Œå³åˆ¤æ–­æ˜¯å¦éœ€è¦è°ƒç”¨å¤–éƒ¨å‡½æ•°æ¥å›ç­”é—®é¢˜
        # è‹¥å­˜åœ¨function_callï¼Œåˆ™æ‰§è¡ŒFunction callingæµç¨‹
        # éœ€è¦è°ƒç”¨å¤–éƒ¨å‡½æ•°ï¼Œç”±äºè€ƒè™‘åˆ°å¯èƒ½å­˜åœ¨å¤šæ¬¡Function callingæƒ…å†µï¼Œè¿™é‡Œåˆ›å»ºWhileå¾ªç¯
        # Whileå¾ªç¯åœæ­¢æ¡ä»¶ï¼šresponse_messageä¸åŒ…å«function_call
        while response_message.tool_calls != None:
            feedback("æ­£åœ¨è°ƒç”¨å¤–éƒ¨å‡½æ•°...")
            # è·å–å‡½æ•°å
            function_name = response_message.tool_calls[0].function.name
            # è·å–å‡½æ•°å¯¹è±¡
            fuction_to_call = available_functions[function_name]
            try:
                # è·å–å‡½æ•°å‚æ•°
                function_args = json.loads(response_message.tool_calls[0].function.arguments)
                # å°†å½“å‰æ“ä½œç©ºé—´ä¸­çš„å…¨å±€å˜é‡æ·»åŠ åˆ°å¤–éƒ¨å‡½æ•°ä¸­
                function_args['g']=globals()
                
                def convert_to_markdown(code, language):
                    return f"```{language}\n{code}\n```"
                
                if function_args.get('py_code'):
                    code = function_args['py_code']
                    markdown_code = convert_to_markdown(code, 'python')
                    feedback("å³å°†æ‰§è¡Œä»¥ä¸‹ä»£ç ï¼š")
        
                else:
                    markdown_code = function_args

                feedback(markdown_code)
                
                if auto_run == False:
                    res = input('è¯·ç¡®è®¤æ˜¯å¦è¿è¡Œä¸Šè¿°ä»£ç ï¼ˆ1ï¼‰ï¼Œæˆ–è€…é€€å‡ºæœ¬æ¬¡è¿è¡Œè¿‡ç¨‹ï¼ˆ2ï¼‰' )

                    if res == '2':
                        print("ç»ˆæ­¢è¿è¡Œ")
                        return None

                feedback("æ­£åœ¨æ‰§è¡Œä»£ç ï¼Œè¯·ç¨å...")

                # å°†å‡½æ•°å‚æ•°è¾“å…¥åˆ°å‡½æ•°ä¸­ï¼Œè·å–å‡½æ•°è®¡ç®—ç»“æœ
                function_response = fuction_to_call(**function_args)

                feedback("å¤–éƒ¨å‡½æ•°å·²è¿è¡Œå®Œæ¯•")

                # messagesä¸­æ‹¼æ¥first responseæ¶ˆæ¯
                messages.append(response_message.model_dump())  
                # messagesä¸­æ‹¼æ¥å‡½æ•°è¾“å‡ºç»“æœ
                messages.append(
                    {
                        "role": "tool",
                        "content": function_response,
                        "tool_call_id":response_message.tool_calls[0].id
                    }
                ) 

                # ç¬¬äºŒæ¬¡è°ƒç”¨æ¨¡å‹
                second_response = get_glm_response(
                    messages=messages, 
                    tools=tools, 
                    model=model,
                    #temperature=0.7,
                    #max_tokens=800,
                    #top_p=0.95,
                    #stream=True
                )  

                       
                # æ›´æ–°response_message
                response_message = second_response.choices[0].message
        
            except Exception as e:
                feedback("jsonæ ¼å¼å¯¹è±¡åˆ›å»ºå¤±è´¥ï¼Œæ­£åœ¨é‡æ–°è¿è¡Œ")
                messages = check_code_run(messages=messages, feedback=feedback,
                                          functions_list = functions_list, 
                                          tools = tools,
                                          model = model, 
                                          auto_run = auto_run)
                return messages
                
    # Whileæ¡ä»¶ä¸æ»¡è¶³ï¼Œæˆ–æ‰§è¡Œå®ŒWhileå¾ªç¯ä¹‹åï¼Œæå–è¿”å›ç»“æœ
    final_response = response_message
    
    #display(Markdown(final_response.content))
    feedback(final_response.content)     
    messages.append(final_response.model_dump())
    
    return messages
    

if "key" not in st.session_state:
    st.session_state.key = None

st.sidebar.markdown("# æ™ºè°±æ¸…è¨€GLMğŸ’")
st.sidebar.header("è‡ªåŠ¨ç¼–ç¨‹æœºå™¨äººğŸ¤–ï¸âš¡ï¸")
st.sidebar.caption("AIæœ¬åœ°åŒ–ä»£ç è§£æå™¨åº”ç”¨")
key = st.sidebar.text_input("Your key", type="password")
model=st.sidebar.selectbox(
    'Model',
    ('glm-3-turbo', 'glm-4', 'glm-4v'))
if not key:
    st.info("Please add your key to continue.")
else:
    st.session_state.key=key    



if not key:
    st.stop()
    
uploaded_files = st.sidebar.file_uploader("Choose upload files", accept_multiple_files=True)  
current_directory = os.getcwd()  
  
for uploaded_file in uploaded_files:  
    file_path = os.path.join(current_directory, uploaded_file.name)  
    with open(file_path, 'wb') as f:  
        f.write(uploaded_file.read())  
    st.write(f'File {uploaded_file.name} saved!')  
    
client = ZhipuAI(api_key=st.session_state.key) 


if "messages" not in st.session_state:
    st.session_state.messages = []

    
for message in st.session_state.messages:
    if "role" in message and (message["role"]=="user" or message["role"]=="assistant" ) and message["content"] is not None:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

def writeReply(cont,msg):
    print(msg)
    cont.write(msg)

if prompt := st.chat_input():
    st.chat_message("user").write(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("assistant"):
        p=st.empty()
        p.write("Thinking...")
        re = check_code_run(st.session_state.messages,lambda x:writeReply(p,x),functions_list = functions_list, 
                               tools = tools,
                               model = model, 
                               auto_run = True)
        print("done!")
        #st.session_state.messages.append({"role": "assistant", "content": re})